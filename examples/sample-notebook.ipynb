{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Hugging Face models on Amazon SageMaker Graviton instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip uninstall -qy autogluon\n",
    "pip install -qU boto3 sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from IPython.display import Markdown, display\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker_streaming import print_event_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "runtime_sm_client = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the deployment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URI of your ECR container (must be in the same region as SageMaker)\n",
    "image_uri = YOUR_IMAGE_URI\n",
    "\n",
    "# The prefix for the endpoint name (a timestamp will be added)\n",
    "endpoint_name_prefix = \"my-graviton-endpoint\"\n",
    "\n",
    "# The instance type for the endpoint (should be a Graviton3/4 instance)\n",
    "real_time_inference_instance_type = \"ml.c7g.8xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A read-only Hugging Face token (only for private or gated models)\n",
    "hf_token = YOUR_HUGGINGFACE_TOKEN\n",
    "\n",
    "# llama-server flags, see 'llama-server -h'\n",
    "llama_cpp_args = \"--ctx-size 16384\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pick a deployment option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: deploy a safetensors model from the HF hub, converting it to GGUF and quantizing it on the fly\n",
    "\n",
    "model_environment = {\n",
    "    # The Hugging Face repository id\n",
    "    \"HF_MODEL_ID\": \"arcee-ai/AFM-4.5B\",\n",
    "    # The quantization recipe to apply\n",
    "    # If left out, the model will be deployed as F16\n",
    "    \"QUANTIZATION\": \"Q8_0\",\n",
    "    # A read-only Hugging Face token (required for private or gated models)\n",
    "    \"HF_TOKEN\": hf_token,\n",
    "    # llama-server flags\n",
    "    'LLAMA_CPP_ARGS': llama_cpp_args\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2: deploy a GGUF model from the HF hub\n",
    "\n",
    "model_environment = {\n",
    "    # The Hugging Face repository id\n",
    "    \"HF_MODEL_ID\": \"arcee-ai/arcee-lite-GGUF\",\n",
    "    # The name of the GGUF file in the repository\n",
    "    \"MODEL_FILENAME\": \"arcee-lite-Q8_0.gguf\",\n",
    "    # llama-server flags\n",
    "    'LLAMA_CPP_ARGS': llama_cpp_args\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 3: deploy a safetensors model from S3, converting it to GGUF and quantizing it on the fly\n",
    "\n",
    "model_environment = {\n",
    "    # The S3 URI of your safetensors model (must be in the same region as SageMaker)\n",
    "    # Downloaded with 'hf download arcee-ai/AFM-4.5B --local-dir afm-4.5-b' and then synced to S3\n",
    "    \"HF_MODEL_URI\": \"s3://YOUR_S3_BUCKET/afm-4.5-b/\",\n",
    "    # The quantization recipe to apply\n",
    "    # If left out, the model will be deployed as F16\n",
    "    \"QUANTIZATION\": \"Q4_0\",\n",
    "    # llama-server flags\n",
    "    \"LLAMA_CPP_ARGS\": llama_cpp_args\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 4: deploy a GGUF model from S3 (the bucket must be in the same region as SageMaker)\n",
    "\n",
    "model_environment = {\n",
    "    # The S3 URI of your GGUF model (must be in the same region as SageMaker)\n",
    "    \"HF_MODEL_URI\": \"s3://YOUR_S3_BUCKET/\",\n",
    "    # The name of the GGUF file in the bucket\n",
    "    # Downloaded with 'hf download arcee-ai/AFM-4.5B-GGUF AFM-4.5B-Q4_0.gguf --local-dir .' and then copied to S3\n",
    "    \"MODEL_FILENAME\": \"AFM-4.5B-Q4_0.gguf\",\n",
    "    # llama-server flags\n",
    "    \"LLAMA_CPP_ARGS\": llama_cpp_args\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployable model\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    env=model_environment,\n",
    ")\n",
    "\n",
    "# create a unique endpoint name\n",
    "timestamp = \"{:%Y-%m-%d-%H-%M-%S}\".format(datetime.datetime.now())\n",
    "endpoint_name = f\"{endpoint_name_prefix}-{timestamp}\"\n",
    "print(f\"Deploying endpoint {endpoint_name}\")\n",
    "\n",
    "# deploy the model\n",
    "response = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=real_time_inference_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_data_download_timeout=900,\n",
    "    container_startup_health_check_timeout=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoint is in service, you will be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run synchronous inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly and helpful AI assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Suggest 5 names for a new neighborhood pet food store. Names should be short, fun, easy to remember, and respectful of pets. \\\n",
    "        Explain why customers would like them.\",\n",
    "        },\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(model_sample_input),\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the generated output with Markdown formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(output[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run streaming inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"As a friendly technical assistant engineer, answer the question in detail.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Why are transformers better models than LSTM? Explain step by step.\"},\n",
    "    ],\n",
    "    \"max_tokens\": 512,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(model_sample_input),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "print_event_stream(response['Body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clean-up\n",
    "\n",
    "Please don't forget to run the cells below to delete all resources and avoid unecessary charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(endpoint_name)\n",
    "model.sagemaker_session.delete_endpoint_config(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
